# ğŸ“Š CarePlus Ticket Analytics Pipeline  
End-to-End Data Engineering Project using AWS (S3, Lambda, Glue, Athena, Redshift) + Power BI

---

## ğŸ“˜ Project Overview

The **CarePlus Ticket Analytics Pipeline** is a complete end-to-end data engineering and analytics solution built using AWS cloud services.

It ingests **support tickets (CSV)** and **application logs (LOG)** into an S3 Data Lake, processes them using **AWS Lambda & AWS Glue**, loads the clean data into **Amazon Redshift**, and visualizes insights using **Power BI**.

This project demonstrates:

- Amazon **S3 (Data Lake)**
- **AWS Lambda** â€” event-driven transformations
- **AWS Glue** â€” ETL and schema management  
- **Amazon Athena** â€” ad-hoc querying  
- **Amazon Redshift** â€” data warehouse  
- **Power BI** â€” dashboard and analytics  

---

## ğŸ—ï¸ Tech Architecture

![Architecture](sandbox:/mnt/data/730ffab8-a720-4e27-aad0-8e6d8fb7b2de.png)

### **Architecture Flow**
1. Extract data from **MySQL (tickets DB)**
2. Store raw data in **S3 Data Lake**
3. Lambda triggers on new files â†’ processes data  
4. Glue crawlers & ETL jobs normalize data  
5. Load into **Redshift Warehouse**
6. Power BI connects to Redshift for reporting  

---

## ğŸ“ S3 Data Lake Folder Structure
s3://careplus-data-lake/
â”œâ”€â”€ support_tickets/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/
â””â”€â”€ support_logs/
â”œâ”€â”€ raw/
â””â”€â”€ processed/

---

## âš™ï¸ ETL Pipeline

### **1ï¸âƒ£ Ingestion (S3)**
- CSV files â†’ `support_tickets/raw/`
- LOG files â†’ `support_logs/raw/`
- S3 event triggers Lambda function

### **2ï¸âƒ£ Transformation (Lambda)**
- Cleans CSV & LOG data  
- Removes duplicates  
- Parses log files  
- Writes clean output to `/processed/`

### **3ï¸âƒ£ AWS Glue**
- Glue Crawler detects schema  
- Glue ETL job further transforms and prepares data  
- Converts into analytical format for Redshift  

### **4ï¸âƒ£ Load to Redshift**

Example Redshift COPY command:

```sql
COPY tickets
FROM 's3://careplus-data-lake/support_tickets/processed/'
IAM_ROLE '<REDSHIFT_ROLE>'
FORMAT AS PARQUET
REGION 'us-east-1';

ğŸ—„ï¸ Redshift Data Warehouse

Final analytical tables include:

tickets

ticket_logs

interactions

agent_performance

These tables enable:

Resolution time analytics

Agent ticket load

Escalation tracking

Issue category analysis

ğŸ“Š Power BI Dashboard

Dashboard Highlights

Total Tickets: 48

Resolved: 39

Open: 8

Escalated: 1

Avg Interactions: 4.38

Avg Resolution Time: 1098 minutes

Tickets by Agent

Tickets by Channel

Resolution Time by Category & Priority

Detailed ticket table

ğŸ¯ What I Learned
AWS

S3 Data Lake design

Lambda event automation

Glue ETL development

Athena for data validation

Redshift warehousing

Data Engineering

Raw vs processed zones

Handling structured & unstructured data

ETL pattern implementation

Data modelling for BI

Visualization

Connecting Power BI with Redshift

Building KPIs, charts, and insights

ğŸš€ How to Run the Pipeline

Upload raw CSV/LOG files to S3

Lambda auto-processes them

Glue crawler updates schema

Glue ETL job prepares final tables

COPY command loads into Redshift

Power BI refreshes dashboard

âœ”ï¸ Conclusion

This project showcases a fully functional cloud-native data analytics pipeline using AWS services and Power BI.

It covers data ingestion â†’ processing â†’ warehousing â†’ visualization, demonstrating strong data engineering and analytics capabilities.
